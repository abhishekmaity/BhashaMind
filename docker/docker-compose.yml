version: "3.9"

services:
  frontend:
    build:
      context: ./frontend
    ports:
      - "3000:3000"
    depends_on:
      api-gateway:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3

  api-gateway:
    build:
      context: ./backend-java
    ports:
      - "8080:8080"
    environment:
      - BACKEND_URL=http://nlp-backend:8000
    depends_on:
      nlp-backend:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  nlp-backend:
    build:
      context: ./backend-python
      dockerfile: Dockerfile.gpu
    ports:
      - "8000:8000"
    environment:
      - USE_CUDA=True
      - REDIS_URL=redis://redis:6379
    runtime: nvidia  # requires NVIDIA Container Toolkit
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

services:
  backend-python:
    build:
      context: ./backend-python
    volumes:
      - model_output:/app/outputs
    environment:
      - OUTPUT_DIR=/app/outputs
    ...

volumes:
  model_output:  # <-- This is the named volume

services:
  rabbitmq:
    image: rabbitmq:3-management
    ports:
      - "5672:5672"     # RabbitMQ
      - "15672:15672"   # Web UI
    environment:
      RABBITMQ_DEFAULT_USER: guest
      RABBITMQ_DEFAULT_PASS: guest
